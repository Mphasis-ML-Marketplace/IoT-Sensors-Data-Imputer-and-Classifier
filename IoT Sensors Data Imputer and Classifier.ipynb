{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, tune, and deploy a custom ML model using IoT Sensors Data Imputer and Classifier Algorithm from AWS Marketplace \n",
    "\n",
    "This solution is a deep learning-based trainable algorithm which can classify data from multiple sensors and impute missing values.\n",
    "\n",
    "This sample notebook shows you how to train a custom ML model using IoT Sensors Data Imputer and Classifier from AWS Marketplace.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "#### Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to For Seller to update: Temperature IoT Data Anomaly Detection. \n",
    "\n",
    "#### Contents:\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)\n",
    "1. [Prepare dataset](#2.-Prepare-dataset)\n",
    "\t1. [Dataset format expected by the algorithm](#A.-Dataset-format-expected-by-the-algorithm)\n",
    "\t1. [Configure and visualize train and test dataset](#B.-Configure-and-visualize-train-and-test-dataset)\n",
    "\t1. [Upload datasets to Amazon S3](#C.-Upload-datasets-to-Amazon-S3)\n",
    "1. [Train a machine learning model](#3:-Train-a-machine-learning-model)\n",
    "\t1. [Set up environment](#3.1-Set-up-environment)\n",
    "\t1. [Train a model](#3.2-Train-a-model)\n",
    "1. [Deploy model and verify results](#4:-Deploy-model-and-verify-results)\n",
    "    1. [Deploy trained model](#A.-Deploy-trained-model)\n",
    "    1. [Create input payload](#B.-Create-input-payload)\n",
    "    1. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "    1. [Visualize output](#D.-Visualize-output)\n",
    "    1. [Calculate relevant metrics](#E.-Calculate-relevant-metrics)\n",
    "    1. [Delete the endpoint](#F.-Delete-the-endpoint)\n",
    "1. [Tune your model! (optional)](#5:-Tune-your-model!-(optional))\n",
    "\t1. [Tuning Guidelines](#A.-Tuning-Guidelines)\n",
    "\t1. [Define Tuning configuration](#B.-Define-Tuning-configuration)\n",
    "\t1. [Run a model tuning job](#C.-Run-a-model-tuning-job)\n",
    "1. [Perform Batch inference](#6.-Perform-Batch-inference)\n",
    "1. [Clean-up](#7.-Clean-up)\n",
    "\t1. [Delete the model](#A.-Delete-the-model)\n",
    "\t1. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "\n",
    "\n",
    "#### Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Subscribe to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the algorithm:\n",
    "1. Open the algorithm listing page IoT Sensors Data Imputer and Classifier.\n",
    "1. On the AWS Marketplace listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify while training a custom ML model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_arn='arn:aws:sagemaker:us-east-2:786796469737:algorithm/mm-imputer-classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json \n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image as ImageEdit\n",
    "import urllib.request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Dataset format expected by the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage Instructions:\n",
    "- The input data should only be in numerical format. String data types will not be supported.\n",
    "- Along with the training file, a text file needs to be uploaded which contains a list of sensors in a list format.\n",
    "- The files should be zipped in a folder named 'train.zip' and uploaded for training\n",
    "- The names of the sensors in this list should also be present in the column names to indicate which sensor a particular column belongs to.\n",
    "- Include data with minimal null/missing values in the training file to ensure that the maximum amount of patterns can be learnt on this clean data.\n",
    "- After training, testing can be done using data with a greater proportion of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm requires data in the format as decribed for best results \n",
    "\n",
    "- The input data should only be in numerical format. There should not be any columns of string data type.\n",
    "- For the initial training data, there should be minimal null/missing values for optimal training performance on maximum clean data from which the model can learn necessary patterns\n",
    "- Null values should not be present as empty strings (\"\" \"\") as these will not be detected during the processing for this solution\n",
    "- Along with the training file, a text file named 'sensor_list.txt' needs to be uploaded which contains a list of sensors in a list format eg. ['sensor_1', 'sensor_2', 'sensor_3'].\n",
    "- The names of the sensors in this list should also be present in the column names to indicate which sensor a particular column belongs to.\n",
    "- Include data with minimal null/missing values in the training file to ensure that the maximum amount of patterns can be learnt on this clean data. Test data can have a greater proportion of missing values.\n",
    "- The expected target values should be present in a column named 'label'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Configure and visualize train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset='training/train.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset='testing/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(test_dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Upload datasets to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sage.Session()\n",
    "bucket=sagemaker_session.default_bucket()\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=sagemaker_session.upload_data(training_dataset, bucket=bucket, key_prefix='iot-sensor-data-imputer')\n",
    "test_data=sagemaker_session.upload_data(test_dataset, bucket=bucket, key_prefix='iot-sensor-data-imputer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training input uploaded to \" + training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Train a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset is available in an accessible Amazon S3 bucket, we are ready to train a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{}/iot-sensor-data-imputer/{}'.format(bucket, 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find more information about hyperparametes in **Hyperparameters** section of IoT Sensors Data Imputer and Classifier Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "hyperparameters={\"epochs\":1,\"batch_size\":10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information on creating an `Estimator` object, see [documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an estimator object for running a training job\n",
    "estimator = sage.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    base_job_name=\"iot-sensor-data-imputer-training\",\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")\n",
    "#Run the training job.\n",
    "# estimator.fit({\"training\": training_data,\"training\":test_data})\n",
    "estimator.fit({\"training\": training_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this [blog-post](https://aws.amazon.com/blogs/machine-learning/easily-monitor-and-visualize-metrics-while-training-models-on-amazon-sagemaker/) for more information how to visualize metrics during the process. You can also open the training job from [Amazon SageMaker console](https://console.aws.amazon.com/sagemaker/home?#/jobs/) and monitor the metrics/logs in **Monitor** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Deploy model and verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can deploy the model for performing real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='iot-sensor-data-imputer-inference'\n",
    "\n",
    "content_type='text/csv'\n",
    "\n",
    "real_time_inference_instance_type='ml.m5.large'\n",
    "batch_transform_inference_instance_type='ml.m5.large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Deploy trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = estimator.deploy(1, real_time_inference_instance_type, serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint is created, you can perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"inference.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"inference.csv\"\n",
    "output_file_name = \"inference_out.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "    --endpoint-name $predictor.endpoint \\\n",
    "    --body fileb://$file_name \\\n",
    "    --content-type $content_type \\\n",
    "    --region $sagemaker_session.boto_region_name \\\n",
    "    $output_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(\"inference_out.csv\", header=None)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. you can terminate the same to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an experiment, you do not need to run a hyperparameter tuning job. However, if you would like to see how to tune a model trained using a third-party algorithm with Amazon SageMaker's hyperparameter tuning functionality, you can run the optional tuning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Perform Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the batch-transform job input files to S3\n",
    "transform_input_folder = \"inference.csv\"\n",
    "transform_input = sagemaker_session.upload_data(transform_input_folder, key_prefix=model_name) \n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the batch-transform job\n",
    "transformer = estimator.transformer(1, batch_transform_inference_instance_type)\n",
    "transformer.transform(transform_input, content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the algorithm, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
